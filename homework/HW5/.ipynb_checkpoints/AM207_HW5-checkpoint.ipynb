{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #5 (Due 10/15/2020, 11:59pm)\n",
    "## Hierarchical Models and the Theory of Variational Inference\n",
    "\n",
    "**AM 207: Advanced Scientific Computing**<br>\n",
    "**Instructor: Weiwei Pan**<br>\n",
    "**Fall 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:**\n",
    "\n",
    "**Students collaborators:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "**Submission Format:** Use this notebook as a template to complete your homework. Please intersperse text blocks (using Markdown cells) amongst `python` code and results -- format your submission for maximum readability. Your assignments will be graded for correctness as well as clarity of exposition and presentation -- a “right” answer by itself without an explanation or is presented with a difficult to follow format will receive no credit.\n",
    "\n",
    "**Code Check:** Before submitting, you must do a \"Restart and Run All\" under \"Kernel\" in the Jupyter or colab menu. Portions of your submission that contains syntactic or run-time errors will not be graded.\n",
    "\n",
    "**Libraries and packages:** Unless a problems specifically asks you to implement from scratch, you are welcomed to use any `python` library package in the standard Anaconda distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Description: Understanding EM and Variational Inference\n",
    "\n",
    "In this problem, we will draw concrete connections between EM and variational inference by applying both methods to a certain class of latent variable models. You'll need to refer to relevant lecture notes on the derivations of EM and the derivation of the variational inference objective. This is an essay question that requires you to engage with complex derivations at a productive but still high level. No implementation is required.\n",
    "\n",
    "#### Non-Bayesian Latent Variable Model\n",
    "Recall the class of latent variable models we studied in lecture:\n",
    "<img src=\"fig/graphical_model.jpg\" style=\"height:150px;\">\n",
    "\n",
    "#### Bayesian Latent Variable Model\n",
    "A Bayesian version of the same class of models involve adding priors for the model parameters:\n",
    "<img src=\"fig/bayesian_model.jpg\" style=\"height:150px;\">\n",
    "\n",
    "1. **(Comparing ELBOs)** For the above type of Bayesian latent variable model, write down the ELBO for variational inference with a mean field variational family. Compare the variational inference ELBO for the Bayesian model to the expectation maximization ELBO for the non-Bayesian model. What are the differences and similarities between these two ELBOs?\n",
    "\n",
    "  In both EM and variational inference we optimize the ELBO. Compare the update steps in EM to the update steps in Coordinate Ascent Variational Inference, draw a concrete analogy between them.\n",
    "  \n",
    "  ***Hint:*** To make both ELBO's comparable, make sure that both are in terms of $z, y, \\theta, \\phi$.\n",
    "  <br><br>\n",
    "  \n",
    "2. **(Comparing ELBOs and KL-divergences)** Recall that the original objective of variational inference is to minimize a KL-divergence, we rewrote the objective to be that of maximizing the ELBO. Why is directly minimizing the KL-divergence in the original objective difficult (be specific about wherein the difficulty lies)? \n",
    "\n",
    "  In the derivation of the E-step of EM, we reframed an maximization of the ELBO problem as a minimization of a KL-divergence problem. In this case, why was the KL-divergence easier to minimize and the ELBO harder to maximize (use the instantiation of the E-step for Gaussian Mixture Models in Lecture 7 to help support your answer)? \n",
    "\n",
    "  In the notes for Lecture 8, we introduce a way to maximize the variational inference ELBO -- through coordinate ascent. In the derivation of the updates for coordinate ascent, there is a place where we reframed an maximization of the ELBO problem as an equivalent minimization of a KL-divergence problem. Write down the exact form of this equivalence (the two expressions are separated in the derivation by a bunch of lines, you'll need to identify both parts that you need). In this case, why was the KL-divergence easier to minimize and the ELBO harder to maximization (use the instantiation of the update for Gaussian Mixture Models in Lecture 8 to help support your answer)?\n",
    "\n",
    "  Based on this analysis, can you draw some general conclusions about when we'd prefer to minimize the KL-divergence versus when we'd prefer to maximize the ELBO?<br><br>\n",
    "\n",
    "3. **(The Mean Field Assumption and Coordinate Ascent)**  Describe exactly when and how the mean field assumption is used in the derivation of the coordinate ascent updates. <br><br>\n",
    "\n",
    "4. **(Generalizability of CAVI)** Summarize what kind of derivations/math is needed in order instantiate Coordinate Ascent Variational Inference (CAVI) for a given new model (look at what we did for Gaussian Mixture Models in Lecture 8 and predict what you'd need to do for a new model). Based on this, discuss the potential draw backs of using CAVI for Bayesian inference in general. Do these draw backs mean that variational inference is not a practical method of inference? What problem(s) need to be solved in order to make variational inference easy to implement for any given Bayesian model?\n",
    "\n",
    "5. **(Generalizability of EM)** Summarize what kind of derivations/math is needed in order instantiate Expectation Maximization (EM) for a given new model (look at what we did for Gaussian Mixture Models in Lecture 9 and predict what you'd need to do for a new model). Based on this, discuss the potential draw backs of using EM for MLE inference in general. Do these draw backs mean that EM is not a practical method of inference? What problem(s) need to be solved in order to make EM easy to implement for any given latent variable model?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Description: Modeling Kidney Cancer Data\n",
    "In this problem, we will continue to work with the US Kidney Cancer Data set, `kcancer.csv`. This is a dataset of kidney cancer frequencies across the US over 5 years on a per county basis. \n",
    "\n",
    "**In this homework, we focus on comparing different types of models for this data set.**\n",
    "\n",
    "\n",
    "## Part I: Empirical Bayes\n",
    "Let $N$ be the number of counties; let $y_j$ the number of kidney cancer case for the $j$-th county, $n_j$ the population of the $j$-county and $\\theta_j$ be the underlying kidney cancer rate for that county. The following is a Bayesian model for our data:\n",
    "\\begin{aligned}\n",
    "y_j | \\theta_j &\\sim Poisson(5 \\cdot n_j \\cdot \\theta_j), \\quad j = 1, \\ldots, N\\\\\n",
    "\\theta_j &\\sim Gamma(\\alpha, \\beta), \\quad j = 1, \\ldots, N\n",
    "\\end{aligned}\n",
    "where $\\alpha, \\beta$ are hyper-parameters of the model.\n",
    "\n",
    "1. **(Visualize the raw cancer rates)** Produce a scatter plot of the raw cancer rates (pct mortality) vs the county population size (in log scale). Highlight the top 300 raw cancer rates in red. Highlight the bottom 300 raw cancer rates in blue. What can you say about the counties with the highest and lowest raw cancer rates.<br><br>\n",
    "\n",
    "2. **(Empirical Bayes)** Using Empirical Bayes and moment matching, choose values for the hyperparameters $\\alpha, \\beta$ based on your data. Use these values of $\\alpha$ and $\\beta$ to obtain posterior distributions for each county.\n",
    "\n",
    "***Hint:*** You'll first need to derive the fact that the ***evidence*** for a Poisson-Gamma model has a Negative Binomial distribution.<br><br>\n",
    "\n",
    "3. **(Justification for Empirical Bayes)** Explain why, in this case, it is not a good idea to pick arbitrary values for $\\alpha$ and $\\beta$ for the gamma prior and proceed with Bayesian inference. Use properties of the cancer dataset to back up your explanation (i.e. it's not *always* problemmatic to choose hyperparameters of the prior arbitrarily, why is it problemmatic for this dataset).<br><br>\n",
    "\n",
    "4. **(Posterior Means)** Produce a scatter plot of the raw cancer rates (pct mortality) vs the county population size (in log scale). Highlight the top 300 raw cancer rates in red. Highlight the bottom 300 raw cancer rates in blue. Finally, on the same plot again, scatter plot the posterior mean cancer rate estimates vs the county population size, highlight these means in green. \n",
    "\n",
    "  Using the scatter plot, explain why using the posterior means (from our model) to estimate cancer rates is preferable to studying the raw rates themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Hierarchical Bayes\n",
    "Rather than choosing fixed constants for the hyperparameters $\\alpha, \\beta$, following the Bayesian philosophy, we typically put additional priors on quantities of which we are uncertain. That is, we model the kidney cancer rates using a ***hierarchical model***:\n",
    "\n",
    "\\begin{aligned}\n",
    "y_j| \\theta_j &\\sim Poisson(5 \\cdot n_j \\cdot \\theta_j), \\quad j = 1, \\ldots, N\\\\\n",
    "\\theta_j | \\alpha, \\beta &\\sim Ga(\\alpha, \\beta), \\quad j = 1, \\ldots, N\\\\\n",
    "\\alpha &\\sim Ga(a, b)\\\\\n",
    "\\beta &\\sim Ga(c, d)\n",
    "\\end{aligned}\n",
    "where $a, b, c, d$ are hyperparameters. \n",
    "\n",
    "1.  **(Posterior Marginal Means)** Produce a scatter plot of the raw cancer rates (pct mortality) vs the county population size (in log scale). Highlight the top 300 raw cancer rates in red. Highlight the bottom 300 raw cancer rates in blue. Finally, on the same plot again, scatter plot the mean of the posterior marginal distribution over $\\theta_j$, i.e. $p(\\theta_j|y_1, \\ldots, y_N)$, vs the county population size (in log scale), highlight these means in orange. \n",
    "\n",
    "  You should use `pymc3` to sample from the posterior. Compare `pymc3`'s sampler with your sampler from the previous homework, what is the difference (if any) in the performance of these samplers?<br><br>\n",
    "\n",
    "2.  **(Hierarchical Bayes vs Empirical Bayes)** Compare the shrinkage of the posterior marginal means of the hierarchical model to the shrinkage of the posterior means from the Bayesian model with empirical Bayes estimates for $\\alpha, \\beta$. What is the difference in shrinkage between the full hierarchical model and the Bayesian model with empirical Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
